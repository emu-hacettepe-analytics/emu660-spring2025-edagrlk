---
title: "Assignment 1"
---

My first assignment has three parts.

## (a)

# A brief summary of Mr. Vardar’s speech

For more details, visit the full [link](https://www.youtube.com/watch?v=Fi8-phj1X1A).

Cem Vardar begins his speech by highlighting the relationship between Industrial Engineering and data science. He emphasizes that Industrial Engineers possess the necessary skills to effectively use data science tools and underscores the synergy between the two disciplines. He explains that data science is not just a technical field but also a powerful tool for problem-solving. Vardar points out that Industrial Engineers have great potential to solve problems in complex systems using data analytics. He stresses that professionals working in data analytics and data science need to possess fundamental tools such as SQL, Python, and Excel, as well as strong analytical thinking and communication skills.

Vardar goes on to discuss the importance of the evolutionary design approach in improving complex systems. He argues that significant changes should be made in small, incremental steps. He also emphasizes the necessity of data-driven decision-making and problem-solving, suggesting that viewing data science merely as a tool will lead to more effective solutions in the business world. Vardar suggests that the combination of Industrial Engineering and data science will create new opportunities in the business world. Finally, he explains that learning just 20% of the skills related to data science (such as data mining, programming languages, and cloud environments) will meet 80% of an Industrial Engineer’s needs and be sufficient for success. This highlights the importance of focusing on the essential skills required to produce effective data-driven solutions in the business world.

## (b)  

# Exploring Statistical Summaries of ‘mtcars’ dataset

```{r}
data(mtcars)

compute_stats <- function(x) {
  c(
    mean = mean(x, na.rm = TRUE),
    median = median(x, na.rm = TRUE),
    variance = var(x, na.rm = TRUE),
    IQR = IQR(x, na.rm = TRUE),
    min = min(x, na.rm = TRUE),
    max = max(x, na.rm = TRUE)
  )
}

result <- sapply(mtcars, compute_stats)

library(knitr)
kable(t(result), caption = "mtcars Data Set Summary")
```

## (c) 

# Handling Missing Values in `na_example` Dataset


```{r}

library(dslabs)  
library(knitr)

data(na_example)

summary(na_example)

missing_count <- sum(is.na(na_example))  

missing_indices <- which(is.na(na_example))  

median_value <- median(na_example, na.rm = TRUE)
na_median_filled <- ifelse(is.na(na_example), median_value, na_example)

summary(na_median_filled)

na_random_filled <- na_example
na_random_filled[is.na(na_random_filled)] <- sample(na_example[!is.na(na_example)], 
                                                     sum(is.na(na_random_filled)), 
                                                     replace = TRUE)

summary(na_random_filled)

comparison <- data.frame(
  Method = c("Original Dataset", "Median Imputation", "Random Imputation"),
  Mean = c(mean(na_example, na.rm = TRUE), mean(na_median_filled, na.rm = TRUE), mean(na_random_filled, na.rm = TRUE)),
  Median = c(median(na_example, na.rm = TRUE), median(na_median_filled, na.rm = TRUE), median(na_random_filled, na.rm = TRUE)),
  Max = c(max(na_example, na.rm = TRUE), max(na_median_filled, na.rm = TRUE), max(na_random_filled, na.rm = TRUE))
)

kable(comparison, caption = "Comparison of Missing Data Handling Methods")

```

## Conclusion
In this analysis, we explored two different methods for handling missing values in the `na_example` dataset:  
- **Median Imputation:** Replacing missing values with the median of the dataset.  
- **Random Imputation:** Filling missing values with randomly selected non-missing values from the dataset.  

The table summarizes the impact of these two methods on the dataset.

### Key Points
- **Median imputation** is a simple and consistent method. However, it reduces variability because all missing values are replaced with the same number.  
- **Random imputation** keeps the dataset’s natural variety but introduces randomness, which may not always be ideal. 
- **Choosing the best method depends on the purpose of the analysis.** If maintaining consistency is more important, median imputation is a better choice. If preserving the original distribution is a priority, random imputation is preferable.  

Choosing the right method depends on the data and what we want to achieve with it.




